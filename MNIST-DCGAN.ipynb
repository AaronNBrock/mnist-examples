{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tensorflow & Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    input_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    input_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    return input_real, input_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, reuse=False, alpha=0.2, training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Layer 0\n",
    "        layer_0 = tf.layers.dense(z, 2*2*128)\n",
    "        layer_0 = tf.reshape(layer_0, (-1, 2, 2, 128))\n",
    "        layer_0 = tf.layers.batch_normalization(layer_0, training=training)\n",
    "        layer_0 = tf.maximum(alpha * layer_0, layer_0)\n",
    "        # Size 2x2x128\n",
    "        \n",
    "        # Layer 1\n",
    "        layer_1 = tf.layers.conv2d_transpose(layer_0, 64, 5, strides=2, padding='valid')\n",
    "        layer_1 = tf.layers.batch_normalization(layer_1, training=training)\n",
    "        layer_1 = tf.maximum(alpha * layer_1, layer_1)\n",
    "        # Size 7x7x64\n",
    "        \n",
    "        # Layer 2\n",
    "        layer_2 = tf.layers.conv2d_transpose(layer_1, 32, 5, strides=2, padding='same')\n",
    "        layer_2 = tf.layers.batch_normalization(layer_2, training=training)\n",
    "        layer_2 = tf.maximum(alpha * layer_2, layer_2)\n",
    "        # Size 14x14x32\n",
    "        \n",
    "        # Output\n",
    "        logits = tf.layers.conv2d_transpose(layer_2, 1, 5, strides=2, padding='same')\n",
    "        out = tf.tanh(logits)\n",
    "        # Size 28x28x1\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2, training=True):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # x: 28x28x1\n",
    "        \n",
    "        # Layer 0 is \n",
    "        layer_0 = tf.layers.conv2d(x, 32, 5, strides=2, padding='same')\n",
    "        layer_0 = tf.maximum(alpha * layer_0, layer_0)\n",
    "        # Size 14x14x32\n",
    "        \n",
    "        # Layer 1\n",
    "        layer_1 = tf.layers.conv2d(layer_0, 64, 5, strides=2, padding='same')\n",
    "        layer_1 = tf.layers.batch_normalization(layer_1, training=training)\n",
    "        layer_1 = tf.maximum(alpha * layer_1, layer_1)\n",
    "        # Size 7x7x64\n",
    "        \n",
    "        # layer_2 = tf.layers.conv2d(layer_1, 128, 5, strides=2, padding='same')\n",
    "        # layer_2 = tf.layers.batch_normalization(layer_2, training=training)\n",
    "        # layer_2 = tf.maximum(alpha * layer_2, layer_2)\n",
    "        # Size 4x4x128\n",
    "\n",
    "        # Flatten it\n",
    "        flat = tf.reshape(layer_1, (-1, 7*7*64))\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = (28, 28, 1)\n",
    "z_size = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "alpha = 0.2\n",
    "# Smoothing\n",
    "smooth = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_real, input_z = model_inputs(input_size, z_size)\n",
    "\n",
    "# Generator\n",
    "with tf.name_scope('generator'):\n",
    "    g_model = generator(input_z, alpha=alpha)\n",
    "\n",
    "# Discriminator\n",
    "with tf.name_scope('discriminator_real'):\n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "with tf.name_scope('discriminator_fake'):\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, alpha=alpha, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('g_loss'):\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "\n",
    "with tf.name_scope('d_loss'):\n",
    "    with tf.name_scope('d_loss_real'):\n",
    "        d_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "    with tf.name_scope('d_loss_fake'):\n",
    "        d_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "\n",
    "    d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/15... Generator Loss: 5.876... Discriminator Loss: 0.396\n",
      "Epoch  2/15... Generator Loss: 4.488... Discriminator Loss: 0.275\n",
      "Epoch  3/15... Generator Loss: 5.776... Discriminator Loss: 0.254\n",
      "Epoch  4/15... Generator Loss: 5.766... Discriminator Loss: 0.236\n",
      "Epoch  5/15... Generator Loss: 6.671... Discriminator Loss: 0.213\n",
      "Epoch  6/15... Generator Loss: 5.251... Discriminator Loss: 0.245\n",
      "Epoch  7/15... Generator Loss: 5.399... Discriminator Loss: 0.233\n",
      "Epoch  8/15... Generator Loss: 6.234... Discriminator Loss: 0.223\n",
      "Epoch  9/15... Generator Loss: 6.009... Discriminator Loss: 0.212\n",
      "Epoch 10/15... Generator Loss: 5.911... Discriminator Loss: 0.216\n",
      "Epoch 11/15... Generator Loss: 4.897... Discriminator Loss: 0.218\n",
      "Epoch 12/15... Generator Loss: 4.349... Discriminator Loss: 0.226\n",
      "Epoch 13/15... Generator Loss: 6.389... Discriminator Loss: 0.218\n",
      "Epoch 14/15... Generator Loss: 6.375... Discriminator Loss: 0.215\n",
      "Epoch 15/15... Generator Loss: 4.837... Discriminator Loss: 0.223\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "losses = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            # Batch Images\n",
    "            batch_images = mnist.train.next_batch(batch_size)[0]\n",
    "            # Random Noise\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Optimize\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "        \n",
    "        g_train_loss = sess.run(g_loss, feed_dict={input_z: batch_z})\n",
    "        d_train_loss = sess.run(d_loss, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "        \n",
    "        print(\n",
    "            \"Epoch {:2d}/{}...\".format(e+1, epochs),\n",
    "            \"Generator Loss: {:.3f}...\".format(g_train_loss),\n",
    "            \"Discriminator Loss: {:.3f}\".format(d_train_loss),\n",
    "        )\n",
    "        \n",
    "        losses.append((g_train_loss, d_train_loss))\n",
    "        \n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(g_model, feed_dict={input_z: sample_z})\n",
    "        \n",
    "        samples.append(gen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Generator')\n",
    "plt.plot(losses.T[1], label='Distriminator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
